#단순 베이지 분류
library(e1071)  #단순 베이즈 분류 패키지
data <- iris    #iris데이터 불러오기

#훈련용 데이터와 검증용 데이터 작성
set.seed(1234) #표본 추출값 고정
idxs <- sample(1:nrow(data), as.integer(0.7*nrow(data)))   #70표본추출
train <- data[idxs,]   #훈련용 데이터 셋
test <- data[-idxs,]  #검증용 데이터세

model <- naiveBayes(Species~.,train)  #단순 베이즈 분류 모형 생성

#실젯값과 예측값의 데이터 프레임 구성
new <- data.frame(실젯값=test$Species) #실젯값
new$예측값 <- predict(model,test) #분류(예측값 생성)

#정오 분류포
predict_table <- table(new$예측값, new$실젯값)  #정오 분류표 생성
names(dimnames(predict_table)) <- c("predicted","observed")
predict_table #정오 분류표

#예측 적중률 산출
new$result <- ifelse(new$실젯값 == new$예측값, "Y", "N")  #관측/예측값 비교결과
predict_prob <- sum(new$result =="Y")/length(new$result)  #예측확률
predict_prob

#신규자료 생성
new_iris <- data.frame(5.1,3.7,1.5,0.3)
names(new_iris) <- c("Sepal.Width", "Sepal.Length", "Petal.Width", "Petal.Lenght")
new_iris

#신규 자료 분류
p_iris <- predict(model, new_iris)  #분류
p_iris



#K-최근접 알고리즘을 사용
library(DMwR)
data <- iris

#훈련용 데이터와 검증용 데이터 작성   #표본 추출값 고정
idxs <- sample(1:nrow(data), as.integer(0.7*nrow(data)))   #data의 70%표본추출
train <- data[idxs,]    #훈련용 데이터 셋
test <- data[-idxs,]    #검증용 데이터 셋

k <- 3   #k값 설정

#k-최근접 이웃 분석 실행
result <- kNN(Species ~., train, test, norm=FALSE, k)

#실제값과 예측값의 데이터 프레임 구성
new <- data.frame(실젯값 = test$Species)  #관측값
new$예측값 <- result   #예측값
predict_table <- table(new$예측값,new$실젯값)
names(dimnames(predict_table)) <- c("predicted", "observed")

predict_table  #정오 분류표

#예측 적증률 산출
new$result <- ifelse(new$실젯값 == new$예측값, "Y", "N")
predict_prob <- sum(new$result == "Y")/length(new$result)  #예측확률
predict_prob



#신규 자료 생성
new_iris <- data.frame(5.1, 3.7, 1.5, 0.3)
names(new_iris) <- c("Sepal.Width", "Sepal.Length", "Petal.Width", "Petal.Length")
new_iris

#신규 자료 분류
p_iris <- kNN(Species ~., train, new_iris, norm=FALSE, k) #분류
p_iris




#서포트 벡터 머신 분석
library(e1071)
data <- iris

#훈련용 데이터와 검증용 데이터 작성
set.seed(1234)    #표본 추출값 고정
idxs <- sample(1:nrow(data), as.integer(0.7*nrow(data)))   #data의 70표본추출
train <- data[idxs,] # 훈련용 데이터 셋
test <- data[-idxs,] # 검증용 데이터 셋
# 서포트 벡터 머신 분석 모형 생성
model <- svm(Species ~ . , train, type = "C-classification", kernel = "radial",
             cost = 10, gamma = 0.1)


# 실젯값과 예측값의 데이터프레임 구성
new <- data.frame(실젯값= test$Species) # 관측값
new$예측값 <- predict(model, test, decision.values = TRUE) # 예측값
# 정오 분류표
predict_table <- table(new$예측값, new$실젯값) # 정오 분류표 작성
names(dimnames(predict_table)) <- c("predicted", "observed")
predict_table
# 예측 적중률 산출
new$result <- ifelse(new$실젯값 == new$예측값, "Y", "N")
predict_prob <- sum(new$result=="Y")/length(new$result) # 예측 확률
predict_prob



# 서포트 벡터 머신 튜닝
cost_range <- 10^(-1:2) # cost 파라미터의 범위
gamma_range <- c(.1,5,1,2) # gamma 파라미터의 범위
svm_tune <- tune(svm, train.x = Species ~ ., data = train, kernel="radial", ranges=list(cost=cost_range,
                                                                                        gamma=gamma_range))
svm_tune
model <- svm(Species ~ . , train, type = "C-classification", kernel = "radial", cost = 1, gamma = 0.1)
new <- data.frame(실젯값= test$ Species) # 관측값
new$예측값 <- predict(model, test, decision.values = TRUE) # 예측값
predict_table <- table(new$예측값, new$실젯값) # 정오 분류표 작성
names(dimnames(predict_table)) <- c("predicted", "observed")
predict_table





# 의사결정나무 분류 분석
library(rpart) # 패키지 불러오기
data <- iris
# 훈련용 데이터와 검증용 데이터 작성
set.seed(1234) # 표본 추출값 고정
idxs <- sample(1:nrow(data), as.integer(0.7*nrow(data))) # data의 70% 표본추출
train <- data[idxs,] # 훈련용 데이터 셋
test <- data[-idxs,] # 검증용 데이터 셋
# 의사결정나무 분류 분석 모형 구축
model <- rpart(Species ~ ., data=train)




# 의사결정나무 분석 모형을 그래픽 지원 패키지 사용
library(rpart.plot) # 패키지 불러오기
rpart.plot(model)
new <- data.frame(실젯값= test$Species) # 실젯값
new$예측값 <- predict(model, newdata=test, type= "class") # 분류(예측값)
# 정오 분류표
predict_table <- table(new$예측값, new$실젯값) # 예측표
names(dimnames(predict_table)) <- c("predicted", "observed")
predict_table
# 예측 적중률 산출
new$result <- ifelse(new$실젯값 == new$예측값, "Y", "N")
predict_prob <- sum(new$result=="Y")/length(new$result) # 예측 확률
predict_prob
